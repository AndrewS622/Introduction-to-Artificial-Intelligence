# CS50AI: Traffic
In this problem set, we were tasked with building an image recognition system for 43 different classes of traffic signs using the TensorFlow and Keras modules in Python. Data are loaded in from a command line argument-specified directory and images are resized to IMG_HEIGHT x IMG_WIDTH x 3 arrays. These images and their corresponding labels are inputs to the convolutional neural network described below. 

Given the large size of the input (2700 elements per image using the standard values for the resized images) and the utility of convolution in computer vision, it is natural to begin the recognition network with a convolutional layer. We'll begin with a simple model, in which we have a single convolutional layer, a max pooling layer, and an output softmax activation function. We can use some example values for the number of convolutional filters (32), filter size (3 x 3), ReLU activation, and pool size in the max-pooling layer (4 x 4). With these parameters, we obtain a test set loss of 0.4535 and accuracy of 93.2%. These are, of course, values dependent on initialization and splitting up the batches, so change slightly between different training runs on the model. We can experiment with many different methods to improve our model, including optimizing hyperparameters, increasing training time, increasing model complexity, and adding regularization or dropout. Let's explore the effect of each of these on model performance. 

Increasing the number of filters by a factor of two in a single convolution layer seems to cause model overfitting- the final accuracy on the training set is 96% and the loss is under 0.14, but the test set loss is 0.58 and the accuracy is 93%. Increasing the filter size in the convolutional layer appears to decrease testing accuracy to around 0.90. Removing the activation function in this layer seems to substantially increase test loss to 1.14 while accuracy remains around 93%, and doubling each dimension of the pooling size similarly increases the loss with a dramatic decline in accuracy. Halving the pooling dimensions seems to overfit the training data. Now we can look at creating a more complex model by adding a second convolution and max pooling layer.

With this modification, we see extremely high loss (2.23) and low accuracy (0.3339), suggesting that we may not have trained our model enough. Increasing training time to 30 epochs rather than 10 partially ameliorates this problem, bringing training accuracy back to 90% but test accuracy only to 88%. If we keep this second layer but reduce it to only 16 filters and a 2 x 2 max-pooling layer, we find that test performance does not change much. If we instead increase the number of filters in the second convolution layer to 64 with only a single 2x2 max-pooling layer, the training takes substantially longer, but our accuracy on the test set is over 97%. By halving the number of filters in each convolution layer and doubling the max-pooling, we can improve training time and still maintain favorable accuracy (> 96%) on the test set. Let's try to improve this loss of accuracy by adding some dense layers. Adding a dropout-regularized fully-connected layer with 10*NUM_CATEGORIES units decreases our training accuracy (due to dropout itself), but keeps test accuracy around 96%. Adding another fully-connected layer does not appear to fix the problem much, so let's instead return to the single dense layer and add another convolution layer, now with 64 filters and a stride of 2. We now get a test accuracy of 94%, suggesting this modification was ineffective. 

Ultimately, we find that larger numbers of filters and larger numbers of convolutional layers allow the network to extract relevant features, at the expense of computation time. With just two convolutional layers with 32 and 64 filters, a 2x2 max-pool layer, and a single densely connected layer prior to the softmax output with 430 units and 0.5 dropout probability, we can achieve >98% test accuracy if we train for 30 epochs rather than 10. Importantly, this works best if we follow typical architectures of CNNs and increase numbers of filters per convolutional layer as the network gets deeper (e.g. from 32 to 64 rather than from 32 to 16 as tested previously). Certainly, more complex examples with extra convolutional layers may potentially increase this accuracy to almost perfect, though the time required to train such a model increases substantially. Conversely, simpler models may potentially have only slight reductions in accuracy with significant decreases in training time. For example, adding a single extra max-pooling layer between the convolutions does this by halving the first two dimensions to the following layers. Given the immense hyperparameter space of even a simple network architecture, it is likely that the optimal design is not explored here, though the test accuracy attained is quite good. 

As a final point, not including dropout in the fully connected layers dramatically hurts performance. Monitoring the training accuracy reveals that, while a small number of fully-connected layers with dropout lead to an accuracy >90% by epoch 8 or so, using a large number of fully-connected layers, at least in this implementation, or not including dropout causes the training accuracy to remain around 5% at least through ten epochs. Test accuracy is equally poor. 

In conclusion, the combined model architecture and hyperparameter space of convolutional neural networks is far too vast to explore here, but increased training time (30 epochs), increasing filter numbers (32 to 64) with layer depth, small numbers of fully connected layers with dropout included, and multiple convolution layers with a single max-pooling layer to reduce parameter counts allows us to perform well on training and testing data, obtaining a final categorical cross-entropy loss of around 0.30 with an accuracy above 98% on testing data. 
